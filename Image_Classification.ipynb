{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80cddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Reshape and one-hot encode the labels\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to initialize weights and biases\n",
    "def initialize_parameters(input_size, hidden_size1, hidden_size2, output_size):\n",
    "    np.random.seed(42)\n",
    "    weights_input_hidden1 = np.random.randn(input_size, hidden_size1)\n",
    "    biases_input_hidden1 = np.zeros((1, hidden_size1))\n",
    "    weights_hidden1_hidden2 = np.random.randn(hidden_size1, hidden_size2)\n",
    "    biases_hidden1_hidden2 = np.zeros((1, hidden_size2))\n",
    "    weights_hidden2_output = np.random.randn(hidden_size2, output_size)\n",
    "    biases_hidden2_output = np.zeros((1, output_size))\n",
    "    return weights_input_hidden1, biases_input_hidden1, weights_hidden1_hidden2, biases_hidden1_hidden2, weights_hidden2_output, biases_hidden2_output\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Initialize the neural network architecture with two hidden layers\n",
    "input_size = x_train.shape[1]\n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 128\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "weights_input_hidden1, biases_input_hidden1, weights_hidden1_hidden2, biases_hidden1_hidden2, weights_hidden2_output, biases_hidden2_output = initialize_parameters(\n",
    "    input_size, hidden_size1, hidden_size2, output_size\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation\n",
    "    hidden1_input = np.dot(x_train, weights_input_hidden1) + biases_input_hidden1\n",
    "    hidden1_output = relu(hidden1_input)\n",
    "    hidden2_input = np.dot(hidden1_output, weights_hidden1_hidden2) + biases_hidden1_hidden2\n",
    "    hidden2_output = relu(hidden2_input)\n",
    "    output_layer_input = np.dot(hidden2_output, weights_hidden2_output) + biases_hidden2_output\n",
    "    output_layer_output = softmax(output_layer_input)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = -1 * np.sum(y_train * np.log(output_layer_output)) / len(x_train)\n",
    "\n",
    "    # Backpropagation\n",
    "    dloss_doutput = output_layer_output - y_train\n",
    "    doutput_dout_layer_input = softmax_derivative(output_layer_output)\n",
    "\n",
    "    # Gradients for the second hidden layer\n",
    "    dout_layer_input_dweights_hidden2_output = hidden2_output.T\n",
    "    dloss_dweights_hidden2_output = np.dot(\n",
    "        dout_layer_input_dweights_hidden2_output, (dloss_doutput * doutput_dout_layer_input)\n",
    "    )\n",
    "    dloss_dbiases_hidden2_output = np.sum(\n",
    "        dloss_doutput * doutput_dout_layer_input, axis=0, keepdims=True\n",
    "    )\n",
    "    dloss_dhidden2_output = np.dot(\n",
    "        dloss_doutput * doutput_dout_layer_input, weights_hidden2_output.T\n",
    "    )\n",
    "    dhidden2_output_dhidden2_input = relu_derivative(hidden2_output)\n",
    "\n",
    "    # Gradients for the first hidden layer\n",
    "    dhidden2_input_dweights_hidden1_hidden2 = hidden1_output.T\n",
    "    dloss_dweights_hidden1_hidden2 = np.dot(\n",
    "        dhidden2_input_dweights_hidden1_hidden2, (dloss_dhidden2_output * dhidden2_output_dhidden2_input)\n",
    "    )\n",
    "    dloss_dbiases_hidden1_hidden2 = np.sum(\n",
    "        dloss_dhidden2_output * dhidden2_output_dhidden2_input, axis=0, keepdims=True\n",
    "    )\n",
    "    dloss_dhidden1_output = np.dot(\n",
    "        dloss_dhidden2_output * dhidden2_output_dhidden2_input, weights_hidden1_hidden2.T\n",
    "    )\n",
    "    dhidden1_output_dhidden1_input = relu_derivative(hidden1_output)\n",
    "\n",
    "    # Gradients for the input layer\n",
    "    dhidden1_input_dweights_input_hidden1 = x_train.T\n",
    "    dloss_dweights_input_hidden1 = np.dot(\n",
    "        dhidden1_input_dweights_input_hidden1, (dloss_dhidden1_output * dhidden1_output_dhidden1_input)\n",
    "    )\n",
    "    dloss_dbiases_input_hidden1 = np.sum(\n",
    "        dloss_dhidden1_output * dhidden1_output_dhidden1_input, axis=0, keepdims=True\n",
    "    )\n",
    "\n",
    "    # Update weights and biases\n",
    "    weights_input_hidden1 -= learning_rate * dloss_dweights_input_hidden1\n",
    "    biases_input_hidden1 -= learning_rate * dloss_dbiases_input_hidden1\n",
    "    weights_hidden1_hidden2 -= learning_rate * dloss_dweights_hidden1_hidden2\n",
    "    biases_hidden1_hidden2 -= learning_rate * dloss_dbiases_hidden1_hidden2\n",
    "    weights_hidden2_output -= learning_rate * dloss_dweights_hidden2_output\n",
    "    biases_hidden2_output -= learning_rate * dloss_dbiases_hidden2_output\n",
    "\n",
    "    # Validation accuracy\n",
    "    #The validation accuracy is the accuracy of the model on the validation set( Validation dataset is a set which is made separately from the training dataset to avoid overfitting) \n",
    "    #It is a measure of how well the model generalizes to new, unseen data.\n",
    "    # Training is often stopped when the validation accuracy stops\n",
    "    #improving or starts decreasing, indicating that further training may lead to overfitting.\n",
    "    correct = 0\n",
    "    for i in range(len(x_val)):\n",
    "        hidden1_input = np.dot(x_val[i], weights_input_hidden1) + biases_input_hidden1\n",
    "        hidden1_output = relu(hidden1_input)\n",
    "        hidden2_input = np.dot(hidden1_output, weights_hidden1_hidden2) + biases_hidden1_hidden2\n",
    "        hidden2_output = relu(hidden2_input)\n",
    "        output_layer_input = np.dot(hidden2_output, weights_hidden2_output) + biases_hidden2_output\n",
    "        output_layer_output = softmax(output_layer_input)\n",
    "        predicted_class = np.argmax(output_layer_output)\n",
    "        true_class = np.argmax(y_val[i])\n",
    "        if predicted_class == true_class:\n",
    "            correct += 1\n",
    "    val_accuracy = correct / len(x_val)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Test accuracy\n",
    "correct = 0\n",
    "for i in range(len(x_test)):\n",
    "    hidden1_input = np.dot(x_test[i], weights_input_hidden1) + biases_input_hidden1\n",
    "    hidden1_output = relu(hidden1_input)\n",
    "    hidden2_input = np.dot(hidden1_output, weights_hidden1_hidden2) + biases_hidden1_hidden2\n",
    "    hidden2_output = relu(hidden2_input)\n",
    "    output_layer_input = np.dot(hidden2_output, weights_hidden2_output) + biases_hidden2_output\n",
    "    output_layer_output = softmax(output_layer_input)\n",
    "    predicted_class = np.argmax(output_layer_output)\n",
    "    true_class = np.argmax(y_test[i])\n",
    "\n",
    "    if predicted_class == true_class:\n",
    "        correct += 1\n",
    "test_accuracy = correct / len(x_test)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060ba9e",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6533344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Importing the data from keras \n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2941b819",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36bba66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Reshaping the data \n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "# In the CIFAR-10 dataset:\n",
    "# The original class labels are integers representing the class index (e.g., 0 for \"airplane,\" 1 for \"automobile,\" etc).\n",
    "# One-hot encoding is used to convert these integer class labels into binary vectors.\n",
    "# For instance, if there are 10 classes in total, the one-hot encoding converts each class label into a 10-dimensional binary vector.\n",
    "# It is used when categorical cross entropy loss is used.\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4446f9d7",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a4a5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data as 80% training data and 20% test data using a function under keras\n",
    "# randomstate is used to specify whether we want the data to be randomly split or it should be recurring if seeded.\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3cccdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function initializes the weights and biases for each layer of the neural network.\n",
    "def initialize_parameters(input_size, hidden_size1, hidden_size2, output_size):\n",
    "    np.random.seed(42)\n",
    "    weights_input_hidden1 = np.random.randn(input_size, hidden_size1)\n",
    "    biases_input_hidden1 = np.zeros((1, hidden_size1))\n",
    "    weights_hidden1_hidden2 = np.random.randn(hidden_size1, hidden_size2)\n",
    "    biases_hidden1_hidden2 = np.zeros((1, hidden_size2))\n",
    "    weights_hidden2_output = np.random.randn(hidden_size2, output_size)\n",
    "    biases_hidden2_output = np.zeros((1, output_size))\n",
    "    return weights_input_hidden1, biases_input_hidden1, weights_hidden1_hidden2, biases_hidden1_hidden2, weights_hidden2_output, biases_hidden2_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3f3d4d",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c64df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0854392",
   "metadata": {},
   "source": [
    "# Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9570784",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = x_train.shape[1]\n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 128\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "weights_input_hidden1, biases_input_hidden1, weights_hidden1_hidden2, biases_hidden1_hidden2, weights_hidden2_output, biases_hidden2_output = initialize_parameters(\n",
    "    input_size, hidden_size1, hidden_size2, output_size\n",
    ")\n",
    "# The above model has two hidden layers and hence two different sets of parameters are initialized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197b3b9",
   "metadata": {},
   "source": [
    "# Train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d17b626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ramachandra\\AppData\\Local\\Temp\\ipykernel_20112\\1459055549.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -1 * np.sum(y_train * np.log(output_layer_output)) / len(x_train)\n",
      "C:\\Users\\Ramachandra\\AppData\\Local\\Temp\\ipykernel_20112\\1459055549.py:16: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -1 * np.sum(y_train * np.log(output_layer_output)) / len(x_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Validation Accuracy: 10.35%\n",
      "Epoch 2/5, Validation Accuracy: 10.16%\n",
      "Epoch 3/5, Validation Accuracy: 10.16%\n"
     ]
    }
   ],
   "source": [
    "# Number of iterations\n",
    "num_epochs = 5\n",
    "\n",
    "# We are using batch gradient descent which uses entire dataset to compute the gradient of the loss function in each iteration.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation\n",
    "    hidden1_input = np.dot(x_train, weights_input_hidden1) + biases_input_hidden1\n",
    "    hidden1_output = relu(hidden1_input)\n",
    "    hidden2_input = np.dot(hidden1_output, weights_hidden1_hidden2) + biases_hidden1_hidden2\n",
    "    hidden2_output = relu(hidden2_input)\n",
    "    output_layer_input = np.dot(hidden2_output, weights_hidden2_output) + biases_hidden2_output\n",
    "    output_layer_output = softmax(output_layer_input)\n",
    "\n",
    "    # Calculate the loss( Categorical Cross entropy loss function)\n",
    "    loss = -1 * np.sum(y_train * np.log(output_layer_output)) / len(x_train)\n",
    "\n",
    "    # Backpropagation\n",
    "    dloss_doutput = output_layer_output - y_train\n",
    "    doutput_dout_layer_input = softmax_derivative(output_layer_output)\n",
    "\n",
    "    # Gradients for the second hidden layer\n",
    "    dout_layer_input_dweights_hidden2_output = hidden2_output.T\n",
    "    dloss_dweights_hidden2_output = np.dot(\n",
    "        dout_layer_input_dweights_hidden2_output, (dloss_doutput * doutput_dout_layer_input)\n",
    "    )\n",
    "    dloss_dbiases_hidden2_output = np.sum(\n",
    "        dloss_doutput * doutput_dout_layer_input, axis=0, keepdims=True\n",
    "    )\n",
    "    dloss_dhidden2_output = np.dot(\n",
    "        dloss_doutput * doutput_dout_layer_input, weights_hidden2_output.T\n",
    "    )\n",
    "    dhidden2_output_dhidden2_input = relu_derivative(hidden2_output)\n",
    "\n",
    "    # Gradients for the first hidden layer\n",
    "    dhidden2_input_dweights_hidden1_hidden2 = hidden1_output.T\n",
    "    dloss_dweights_hidden1_hidden2 = np.dot(\n",
    "        dhidden2_input_dweights_hidden1_hidden2, (dloss_dhidden2_output * dhidden2_output_dhidden2_input)\n",
    "    )\n",
    "    dloss_dbiases_hidden1_hidden2 = np.sum(\n",
    "        dloss_dhidden2_output * dhidden2_output_dhidden2_input, axis=0, keepdims=True\n",
    "    )\n",
    "    dloss_dhidden1_output = np.dot(\n",
    "        dloss_dhidden2_output * dhidden2_output_dhidden2_input, weights_hidden1_hidden2.T\n",
    "    )\n",
    "    dhidden1_output_dhidden1_input = relu_derivative(hidden1_output)\n",
    "\n",
    "    # Gradients for the input layer\n",
    "    dhidden1_input_dweights_input_hidden1 = x_train.T\n",
    "    dloss_dweights_input_hidden1 = np.dot(\n",
    "        dhidden1_input_dweights_input_hidden1, (dloss_dhidden1_output * dhidden1_output_dhidden1_input)\n",
    "    )\n",
    "    dloss_dbiases_input_hidden1 = np.sum(\n",
    "        dloss_dhidden1_output * dhidden1_output_dhidden1_input, axis=0, keepdims=True\n",
    "    )\n",
    "\n",
    "    # Update weights and biases\n",
    "    weights_input_hidden1 -= learning_rate * dloss_dweights_input_hidden1\n",
    "    biases_input_hidden1 -= learning_rate * dloss_dbiases_input_hidden1\n",
    "    weights_hidden1_hidden2 -= learning_rate * dloss_dweights_hidden1_hidden2\n",
    "    biases_hidden1_hidden2 -= learning_rate * dloss_dbiases_hidden1_hidden2\n",
    "    weights_hidden2_output -= learning_rate * dloss_dweights_hidden2_output\n",
    "    biases_hidden2_output -= learning_rate * dloss_dbiases_hidden2_output\n",
    "\n",
    "    # Validation accuracy\n",
    "    #The validation accuracy is the accuracy of the model on the validation set( Validation dataset is a set which is made separately from the training dataset to avoid overfitting) \n",
    "    #It is a measure of how well the model generalizes to new, unseen data.\n",
    "    # Training is often stopped when the validation accuracy stops\n",
    "    #improving or starts decreasing, indicating that further training may lead to overfitting.\n",
    "    correct = 0\n",
    "    for i in range(len(x_val)):\n",
    "        hidden1_input = np.dot(x_val[i], weights_input_hidden1) + biases_input_hidden1\n",
    "        hidden1_output = relu(hidden1_input)\n",
    "        hidden2_input = np.dot(hidden1_output, weights_hidden1_hidden2) + biases_hidden1_hidden2\n",
    "        hidden2_output = relu(hidden2_input)\n",
    "        output_layer_input = np.dot(hidden2_output, weights_hidden2_output) + biases_hidden2_output\n",
    "        output_layer_output = softmax(output_layer_input)\n",
    "        predicted_class = np.argmax(output_layer_output)\n",
    "        true_class = np.argmax(y_val[i])\n",
    "        if predicted_class == true_class:\n",
    "            correct += 1\n",
    "    val_accuracy = correct / len(x_val)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Accuracy: {val_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c288ce18",
   "metadata": {},
   "source": [
    "# Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c33771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "correct = 0\n",
    "for i in range(len(x_test)):\n",
    "    hidden1_input = np.dot(x_test[i], weights_input_hidden1) + biases_input_hidden1\n",
    "    hidden1_output = relu(hidden1_input)\n",
    "    hidden2_input = np.dot(hidden1_output, weights_hidden1_hidden2) + biases_hidden1_hidden2\n",
    "    hidden2_output = relu(hidden2_input)\n",
    "    output_layer_input = np.dot(hidden2_output, weights_hidden2_output) + biases_hidden2_output\n",
    "    output_layer_output = softmax(output_layer_input)\n",
    "    predicted_class = np.argmax(output_layer_output)\n",
    "    true_class = np.argmax(y_test[i])\n",
    "\n",
    "    if predicted_class == true_class:\n",
    "        correct += 1\n",
    "test_accuracy = correct / len(x_test)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31afdcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
